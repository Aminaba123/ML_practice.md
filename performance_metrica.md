For PCA and LDA 

Model Accuracy and Performance Check

When it comes to checking the model accuracy, you can use the below techniques to see how well the model is performing.

`Confusion Matrix`: A confusion matrix is a table that is used to evaluate the performance of a classification model. It is a table of the true positive, true negative, false positive, and false negative values for a classification problem. It is also used to calculate various metrics such as precision, recall, accuracy, and f1-score.

`Accuracy Score`: The accuracy score is the ratio of correctly predicted observations to the total observations. It is a primary evaluation metric for classification problems.

`F1 Score`: F1 score is the harmonic mean of precision and recall. It is usually used to compare two or more classifiers.

`ROC Curve`: A ROC curve is a graphical representation of the performance of a classification model. It is a plot of the true positive rate against the false positive rate. The higher the area under the curve, the better the model is at predicting the positive class.

